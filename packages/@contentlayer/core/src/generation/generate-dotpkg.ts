import type { AbsolutePosixFilePath } from '@contentlayer/utils'
import { filePathJoin } from '@contentlayer/utils'
import * as utils from '@contentlayer/utils'
import type { E, HasClock, HasConsole } from '@contentlayer/utils/effect'
import { Array, Chunk, flow, OT, pipe, S, T } from '@contentlayer/utils/effect'
import { fs } from '@contentlayer/utils/node'
import { camelCase } from 'camel-case'
import type { PackageJson } from 'type-fest'

import { ArtifactsDir } from '../ArtifactsDir.js'
import type { HasCwd } from '../cwd.js'
import type { DataCache } from '../DataCache.js'
import type { SourceProvideSchemaError } from '../errors.js'
import type { Config } from '../getConfig/index.js'
import type { SourceFetchDataError } from '../index.js'
import type { PluginOptions, SourcePluginType } from '../plugin.js'
import type { DocumentTypeDef, SchemaDef } from '../schema/index.js'
import { autogeneratedNote } from './common.js'
import { renderTypes } from './generate-types.js'

/**
 * Used to track which files already have been written.
 * Gets re-initialized per `generateDotpkg` invocation therefore only "works" during dev mode.
 */
type FilePath = string
type DocumentHash = string
type WrittenFilesCache = Record<FilePath, DocumentHash>

export type GenerationOptions = {
  sourcePluginType: SourcePluginType
  options: PluginOptions
}

type GenerateDotpkgError =
  | fs.WriteFileError
  | fs.JsonStringifyError
  | fs.MkdirError
  | fs.RmError
  | SourceProvideSchemaError
  | SourceFetchDataError

export type GenerateInfo = {
  documentCount: number
}

export const logGenerateInfo = (info: GenerateInfo): T.Effect<HasConsole, never, void> =>
  T.log(`Generated ${info.documentCount} documents in .contentlayer`)

export const generateDotpkg = ({
  config,
  verbose,
}: {
  config: Config
  verbose: boolean
}): T.Effect<OT.HasTracer & HasClock & HasCwd & HasConsole, GenerateDotpkgError, GenerateInfo> =>
  pipe(
    generateDotpkgStream({ config, verbose, isDev: false }),
    S.take(1),
    S.runCollect,
    T.map(Chunk.unsafeHead),
    T.rightOrFail,
    OT.withSpan('@contentlayer/core/generation:generateDotpkg', { attributes: { verbose } }),
  )

// TODO make sure unused old generated files are removed
export const generateDotpkgStream = ({
  config,
  verbose,
  isDev,
}: {
  config: Config
  verbose: boolean
  isDev: boolean
}): S.Stream<OT.HasTracer & HasClock & HasCwd & HasConsole, never, E.Either<GenerateDotpkgError, GenerateInfo>> => {
  const writtenFilesCache = {}
  const generationOptions = { sourcePluginType: config.source.type, options: config.source.options }
  const resolveParams = pipe(
    T.structPar({
      schemaDef: config.source.provideSchema(config.esbuildHash),
      targetPath: ArtifactsDir.mkdir,
    }),
    T.either,
  )

  // .pipe(
  //   tap((artifactsDir) => watchData && errorIfArtifactsDirIsDeleted({ artifactsDir }))
  // ),

  return pipe(
    S.fromEffect(resolveParams),
    S.chainMapEitherRight(({ schemaDef, targetPath }) =>
      pipe(
        config.source.fetchData({ schemaDef, verbose }),
        S.mapEffectEitherRight((cache) =>
          pipe(
            writeFilesForCache({ schemaDef, targetPath, cache, generationOptions, writtenFilesCache, isDev }),
            T.eitherMap(() => ({ documentCount: Object.keys(cache.cacheItemsMap).length })),
          ),
        ),
      ),
    ),
  )
}

const writeFilesForCache = ({
  cache,
  schemaDef,
  targetPath,
  generationOptions,
  writtenFilesCache,
  isDev,
}: {
  schemaDef: SchemaDef
  cache: DataCache.Cache
  targetPath: AbsolutePosixFilePath
  generationOptions: GenerationOptions
  writtenFilesCache: WrittenFilesCache
  isDev: boolean
}): T.Effect<
  OT.HasTracer,
  never,
  E.Either<fs.WriteFileError | fs.MkdirError | fs.RmError | fs.JsonStringifyError, void>
> =>
  pipe(
    T.gen(function* ($) {
      const withPrefix = (...path_: string[]) => filePathJoin(targetPath, ...path_)

      if (process.env['CL_DEBUG']) {
        yield* $(fs.mkdirp(withPrefix('.cache')))
        yield* $(
          T.collectAllPar([
            fs.writeFileJson({ filePath: withPrefix('.cache', 'schema.json'), content: schemaDef as any }),
            fs.writeFileJson({ filePath: withPrefix('.cache', 'data-cache.json'), content: cache }),
          ]),
        )
      }

      const allCacheItems = Object.values(cache.cacheItemsMap)
      const allDocuments = allCacheItems.map((_) => _.document)

      const documentDefs = Object.values(schemaDef.documentTypeDefMap)

      const [nodeVersionMajor, nodeVersionMinor] = yield* $(
        T.succeedWith(() => process.versions.node.split('.').map((_) => parseInt(_, 10)) as [number, number, number]),
      )

      // NOTE Type assert statements for `.json` files are neccessary from Node v16.14 onwards
      const needsJsonAssertStatement = nodeVersionMajor > 16 || (nodeVersionMajor === 16 && nodeVersionMinor >= 14)
      const assertStatement = needsJsonAssertStatement ? ` assert { type: 'json' }` : ''

      const typeNameField = generationOptions.options.fieldOptions.typeFieldName
      const dataBarrelFiles = documentDefs.map((docDef) => ({
        content: makeDataExportFile({
          docDef,
          documentIds: allDocuments.filter((_) => _[typeNameField] === docDef.name).map((_) => _._id),
          assertStatement,
        }),
        filePath: withPrefix('generated', docDef.name, `_index.mjs`),
      }))

      const individualDataJsonFiles = allCacheItems.map(({ document, documentHash }) => ({
        content: JSON.stringify(document, null, 2),
        filePath: withPrefix('generated', document[typeNameField], `${idToFileName(document._id)}.json`),
        documentHash,
      }))

      const collectionDataJsonFiles = pipe(
        documentDefs,
        Array.map((documentDef) => {
          const documents = allDocuments.filter((_) => _[typeNameField] === documentDef.name)
          const jsonData = documentDef.isSingleton ? documents[0]! : documents

          return {
            content: JSON.stringify(jsonData, null, 2),
            filePath: withPrefix('generated', documentDef.name, `_index.json`),
            documentHash: documents.map((_) => _.documentHash).join(''),
          }
        }),
      )

      const dataDirPaths = documentDefs.map((_) => withPrefix('generated', _.name))
      yield* $(T.forEachPar_([withPrefix('generated'), ...dataDirPaths], fs.mkdirp))

      const writeFile = writeFileWithWrittenFilesCache({ writtenFilesCache })

      yield* $(
        T.collectAllPar([
          writeFile({ filePath: withPrefix('package.json'), content: makePackageJson(schemaDef.hash) }),
          writeFile({
            filePath: withPrefix('generated', 'types.d.ts'),
            content: renderTypes({ schemaDef, generationOptions }),
            rmBeforeWrite: true,
          }),
          writeFile({
            filePath: withPrefix('generated', 'index.d.ts'),
            content: makeDataTypes({ schemaDef }),
            rmBeforeWrite: true,
          }),
          writeFile({
            filePath: withPrefix('generated', 'index.mjs'),
            content: makeIndexMjs({ schemaDef, assertStatement, isDev }),
          }),
          ...dataBarrelFiles.map(writeFile),
          ...individualDataJsonFiles.map(writeFile),
          ...collectionDataJsonFiles.map(writeFile),
          // TODO generate readme file
        ]),
      )
    }),
    OT.withSpan('@contentlayer/core/generation/generate-dotpkg:writeFilesForCache', {
      attributes: {
        targetPath,
        cacheKeys: Object.keys(cache.cacheItemsMap),
      },
    }),
    T.either,
  )

const makePackageJson = (schemaHash: string): string => {
  const packageJson: PackageJson & { typesVersions: any } = {
    name: 'dot-contentlayer',
    description: 'This package is auto-generated by Contentlayer',
    // TODO generate more meaningful version (e.g. by using Contentlayer version and schema hash)
    version: `0.0.0-${schemaHash}`,
    exports: {
      './generated': {
        import: './generated/index.mjs',
      },
    },
    typesVersions: {
      '*': {
        generated: ['./generated'],
      },
    },
  }

  return JSON.stringify(packageJson, null, 2)
}

/**
 * Remembers which files already have been written to disk.
 * If no `documentHash` was provided, the writes won't be cached.
 *
 * TODO maybe rewrite with effect-cache
 */
const writeFileWithWrittenFilesCache =
  ({ writtenFilesCache }: { writtenFilesCache: WrittenFilesCache }) =>
  ({
    filePath,
    content,
    documentHash,
    rmBeforeWrite = true,
  }: {
    filePath: AbsolutePosixFilePath
    content: string
    documentHash?: string
    /** In order for VSC to pick up changes in generated files, it's currently needed to delete the file before re-creating it */
    rmBeforeWrite?: boolean
  }) =>
    T.gen(function* ($) {
      // TODO also consider schema hash
      const fileIsUpToDate = documentHash !== undefined && writtenFilesCache[filePath] === documentHash
      if (!rmBeforeWrite && fileIsUpToDate) {
        return
      }

      if (rmBeforeWrite) {
        yield* $(fs.rm(filePath, { force: true }))
      }
      yield* $(fs.writeFile(filePath, content))
      if (documentHash) {
        writtenFilesCache[filePath] = documentHash
      }
    })

const makeDataExportFile = ({
  docDef,
  documentIds,
  assertStatement,
}: {
  docDef: DocumentTypeDef
  documentIds: string[]
  assertStatement: string
}): string => {
  const dataVariableName = getDataVariableName({ docDef })

  if (docDef.isSingleton) {
    const documentId = documentIds[0]!
    return `\
// ${autogeneratedNote}
export { default as ${dataVariableName} } from './${idToFileName(documentId)}.json'${assertStatement}
`
  }

  const makeVariableName = flow(idToFileName, (_) => camelCase(_, { stripRegexp: /[^A-Z0-9\_]/gi }))

  const docImports = documentIds
    .map((_) => `import ${makeVariableName(_)} from './${idToFileName(_)}.json'${assertStatement}`)
    .join('\n')

  return `\
// ${autogeneratedNote}

${docImports}

export const ${dataVariableName} = [${documentIds.map((_) => makeVariableName(_)).join(', ')}]
`
}

const makeIndexMjs = ({
  schemaDef,
  assertStatement,
  isDev,
}: {
  schemaDef: SchemaDef
  assertStatement: string
  isDev: boolean
}): string => {
  const dataVariableNames = Object.values(schemaDef.documentTypeDefMap).map((docDef) => ({
    isSingleton: docDef.isSingleton,
    documentDefName: docDef.name,
    dataVariableName: getDataVariableName({ docDef }),
  }))

  const constExports = 'export { ' + dataVariableNames.map((_) => _.dataVariableName).join(', ') + ' }'

  const constImportsForAllDocuments = dataVariableNames
    .map(({ documentDefName, dataVariableName }) =>
      isDev
        ? `import { ${dataVariableName} } from './${documentDefName}/_index.mjs'`
        : `import ${dataVariableName} from './${documentDefName}/_index.json'${assertStatement}`,
    )
    .join('\n')

  const allDocuments = dataVariableNames
    .map(({ isSingleton, dataVariableName }) => (isSingleton ? dataVariableName : `...${dataVariableName}`))
    .join(', ')

  return `\
// ${autogeneratedNote}

export { isType } from 'contentlayer/client'

// NOTE During development Contentlayer imports from \`.mjs\` files to improve HMR speeds.
// During (production) builds Contentlayer it imports from \`.json\` files to improve build performance.
${constImportsForAllDocuments}

${constExports}

export const allDocuments = [${allDocuments}]
`
}

export const makeDataTypes = ({ schemaDef }: { schemaDef: SchemaDef }): string => {
  const dataConsts = Object.values(schemaDef.documentTypeDefMap)
    .map((docDef) => [docDef, docDef.name, getDataVariableName({ docDef })] as const)
    .map(
      ([docDef, typeName, dataVariableName]) =>
        `export declare const ${dataVariableName}: ${typeName}${docDef.isSingleton ? '' : '[]'}`,
    )
    .join('\n')

  const documentTypeNames = Object.values(schemaDef.documentTypeDefMap)
    .map((docDef) => docDef.name)
    .join(', ')

  return `\
// ${autogeneratedNote}

import { ${documentTypeNames}, DocumentTypes } from './types'

export * from './types'

${dataConsts}

export declare const allDocuments: DocumentTypes[]

`
}

const getDataVariableName = ({ docDef }: { docDef: DocumentTypeDef }): string => {
  if (docDef.isSingleton) {
    return utils.lowercaseFirstChar(utils.inflection.singularize(docDef.name))
  } else {
    return 'all' + utils.uppercaseFirstChar(utils.inflection.pluralize(docDef.name))
  }
}

const idToFileName = (id: string): string => leftPadWithUnderscoreIfStartsWithNumber(id).replace(/\//g, '__')

const leftPadWithUnderscoreIfStartsWithNumber = (str: string): string => {
  if (/^[0-9]/.test(str)) {
    return '_' + str
  }
  return str
}

// const errorIfArtifactsDirIsDeleted = ({ artifactsDir }: { artifactsDir: string }) => {
//   watch(artifactsDir, async (event) => {
//     if (event === 'rename' && !(await fileOrDirExists(artifactsDir))) {
//       console.error(`Seems like the target directory (${artifactsDir}) was deleted. Please restart the command.`)
//       process.exit(1)
//     }
//   })
// }
